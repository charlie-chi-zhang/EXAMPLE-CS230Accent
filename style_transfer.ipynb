{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Style Transfer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named tensorflow",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b5c837f599ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#from nst_utils import *\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named tensorflow"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "#from nst_utils import *\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the content cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_content_cost(a_C, a_G):\n",
    "    \"\"\"\n",
    "    Computes the content cost\n",
    "    \n",
    "    Arguments:\n",
    "    a_C -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image C \n",
    "    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image G\n",
    "    \n",
    "    Returns: \n",
    "    J_content -- scalar that you compute using equation 1 above.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from a_G (≈1 line)\n",
    "    m, n_H, n_W, n_C = a_G.shape().as_list()\n",
    "    \n",
    "    # Reshape a_C and a_G (≈2 lines)\n",
    "    a_C_unrolled = tf.transpose(tf.reshape(a_C, [tf.to_int32(m), tf.to_int32(n_H*n_W), tf.to_int32(n_C)]), perm=[0, 2, 1])\n",
    "    a_G_unrolled = tf.transpose(tf.reshape(a_G, [tf.to_int32(m), tf.to_int32(n_H*n_W), tf.to_int32(n_C)]), perm=[0, 2, 1])\n",
    "\n",
    "    # compute the cost with tensorflow (≈1 line)\n",
    "    x = tf.subtract(a_C_unrolled, a_G_unrolled)\n",
    "    y = tf.square(x)\n",
    "    z = tf.reduce_sum(y)\n",
    "    J_content = tf.multiply(tf.to_float(tf.divide(tf.to_int32(1), tf.multiply(tf.to_int32(4), tf.multiply(tf.multiply(n_H,n_W), n_C)))), z)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return J_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the style cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Style matrix (Gram matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(A):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    A -- matrix of shape (n_C, n_H*n_W)\n",
    "    \n",
    "    Returns:\n",
    "    GA -- Gram matrix of A, of shape (n_C, n_C)\n",
    "    \"\"\"\n",
    "\n",
    "    GA = tf.matmul(A, tf.transpose(A))\n",
    "    \n",
    "    return GA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Style cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layer_style_cost(a_S, a_G):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    a_S -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image S \n",
    "    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image G\n",
    "    \n",
    "    Returns: \n",
    "    J_style_layer -- tensor representing a scalar value, style cost defined above by equation (2)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from a_G\n",
    "    m, n_H, n_W, n_C = a_G.get_shape().as_list()\n",
    "    \n",
    "    # Reshape the images to have them of shape (n_C, n_H*n_W)\n",
    "    a_S = tf.transpose(tf.reshape(a_S, [tf.to_int32(m), tf.to_int32(n_H*n_W), tf.to_int32(n_C)]), perm=[0, 2, 1])\n",
    "    a_G = tf.transpose(tf.reshape(a_G, [tf.to_int32(m), tf.to_int32(n_H*n_W), tf.to_int32(n_C)]), perm=[0, 2, 1])\n",
    "\n",
    "    # Computing gram_matrices for both images S and G\n",
    "    GS = gram_matrix(a_S[0])\n",
    "    GG = gram_matrix(a_G[0])\n",
    "\n",
    "    # Computing the loss\n",
    "    x = tf.subtract(GS, GG)\n",
    "    y = tf.square(x)\n",
    "    z = tf.reduce_sum(y)\n",
    "    nh_nw = tf.multiply(n_H,n_W)\n",
    "    denominator = tf.multiply(tf.constant(4), tf.multiply(tf.multiply(nh_nw, nh_nw), tf.multiply(n_C, n_C)))\n",
    "    divided = tf.to_float(tf.divide(tf.constant(1), denominator))\n",
    "    J_style_layer = tf.multiply(divided, z)\n",
    "    \n",
    "    return J_style_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute style cost of several layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_style_cost(model, STYLE_LAYERS):\n",
    "    \"\"\"\n",
    "    Computes the overall style cost from several chosen layers\n",
    "    \n",
    "    Arguments:\n",
    "    model -- our tensorflow model\n",
    "    STYLE_LAYERS -- A python list containing:\n",
    "                        - the names of the layers we would like to extract style from\n",
    "                        - a coefficient for each of them\n",
    "    \n",
    "    Returns: \n",
    "    J_style -- tensor representing a scalar value, style cost defined above by equation (2)\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize the overall style cost\n",
    "    J_style = 0\n",
    "\n",
    "    for layer_name, coeff in STYLE_LAYERS:\n",
    "\n",
    "        # Select the output tensor of the currently selected layer\n",
    "        out = model[layer_name]\n",
    "\n",
    "        # Set a_S to be the hidden layer activation from the layer we have selected, by running the session on out\n",
    "        a_S = sess.run(out)\n",
    "\n",
    "        # Set a_G to be the hidden layer activation from same layer. Here, a_G references model[layer_name] \n",
    "        # and isn't evaluated yet. Later in the code, we'll assign the image G as the model input, so that\n",
    "        # when we run the session, this will be the activations drawn from the appropriate layer, with G as input.\n",
    "        a_G = out\n",
    "        \n",
    "        # Compute style_cost for the current layer\n",
    "        J_style_layer = compute_layer_style_cost(a_S, a_G)\n",
    "\n",
    "        # Add coeff * J_style_layer of this layer to overall style cost\n",
    "        J_style += coeff * J_style_layer\n",
    "\n",
    "    return J_style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total cost to optimize\n",
    "Cost function that minimizes both content and style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_cost(J_content, J_style, alpha = 10, beta = 40):\n",
    "    \"\"\"\n",
    "    Computes the total cost function\n",
    "    \n",
    "    Arguments:\n",
    "    J_content -- content cost coded above\n",
    "    J_style -- style cost coded above\n",
    "    alpha -- hyperparameter weighting the importance of the content cost\n",
    "    beta -- hyperparameter weighting the importance of the style cost\n",
    "    \n",
    "    Returns:\n",
    "    J -- total cost as defined by the formula above.\n",
    "    \"\"\"\n",
    "    \n",
    "    J = alpha*J_content + beta*J_style\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the Neural Style Transfer\n",
    "1. Create an Interactive Session\n",
    "2. Load the content image\n",
    "3. Load the style image\n",
    "4. Randomly initialize the image to be generated\n",
    "5. Load the VGG16 model\n",
    "6. Build the TensorFlow graph:\n",
    "7. Run the content image through the VGG16 model and compute the content cost\n",
    "8. Run the style image through the VGG16 model and compute the style cost\n",
    "9. Compute the total cost\n",
    "10. Define the optimizer and the learning rate\n",
    "11. Initialize the TensorFlow graph and run it for a large number of iterations, updating the generated image at every step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model NN\n",
    "Function initializes the variables of the tensorflow graph, assigns the input image (initial generated image) as the input of the model, and runs the train_step for a large number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_nn(sess, input_spectrogram, num_iterations = 200):\n",
    "    \n",
    "    # Initialize global variables (you need to run the session on the initializer)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Run the noisy input image (initial generated image) through the model.\n",
    "    sess.run(model['input'].assign(input_spectrogram))\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "    \n",
    "        # Run the session on the train_step to minimize the total cost\n",
    "        sess.run(train_step) \n",
    "        \n",
    "        # Compute the generated image by running the session on the current model['input']\n",
    "        generated_spectrogram = sess.run(model['input'])\n",
    "\n",
    "        # Print every 20 iteration.\n",
    "        if i%20 == 0:\n",
    "            Jt, Jc, Js = sess.run([J, J_content, J_style])\n",
    "            print(\"Iteration \" + str(i) + \" :\")\n",
    "            print(\"total cost = \" + str(Jt))\n",
    "            print(\"content cost = \" + str(Jc))\n",
    "            print(\"style cost = \" + str(Js))\n",
    "            \n",
    "            # save current generated spectogram in the \"/output\" directory\n",
    "            #save_image(\"output/\" + str(i) + \".npy\", generated_spectrogram)\n",
    "    \n",
    "    # save last generated image\n",
    "    save_image('output/generated_spectrogram.npy', generated_spectrogram)\n",
    "    \n",
    "    return generated_spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load saved model\n",
    "Returns the CNN model stored in a python dictionary where each variable name is the key and the corresponding value is a tensor containing that variable's value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cnn_model(path, spectrogram_shape):\n",
    "    \"\"\"\n",
    "    Here is the detailed configuration of the CNN model:\n",
    "    0 is conv1_1 (5, 5, 1, 32)\n",
    "    1 is relu\n",
    "    2 is maxpool\n",
    "    3 is conv2_1 (3, 3, 32, 64)\n",
    "    4 is relu\n",
    "    5 is maxpool\n",
    "    6 is conv3_1 (3, 3, 32, 64)\n",
    "    7 is relu\n",
    "    8 is maxpool\n",
    "    9 is conv4_1 (3, 3, 32, 64)\n",
    "    10 is relu\n",
    "    11 is maxpool\n",
    "    12 is conv5_1 (3, 3, 32, 64)\n",
    "    13 is relu\n",
    "    14 is maxpool\n",
    "    15 is conv6_1 (3, 3, 32, 64)\n",
    "    16 is relu\n",
    "    17 is maxpool\n",
    "    18 is conv7_1 (3, 3, 32, 64)\n",
    "    19 is relu\n",
    "    20 is maxpool\n",
    "    21 is conv8_1 (3, 3, 32, 64)\n",
    "    22 is relu\n",
    "    23 is maxpool\n",
    "    24 is conv9_1 (3, 3, 32, 64)\n",
    "    25 is relu\n",
    "    26 is maxpool\n",
    "    27 is conv10_1 (3, 3, 32, 64)\n",
    "    28 is relu\n",
    "    29 is maxpool\n",
    "    30 is flatten\n",
    "    31 is fullyconnected\n",
    "    32 is softmax\n",
    "    \"\"\"\n",
    "    \n",
    "    cnn = scipy.io.loadmat(path)\n",
    "    cnn_layers = cnn['layers']\n",
    "    \n",
    "    def _weights(layer, expected_layer_name):\n",
    "        \"\"\"\n",
    "        Return the weights and bias from the CNN model for a given layer.\n",
    "        \"\"\"\n",
    "        wb = cnn_layers[0][layer][0][0][2]\n",
    "        W = wb[0][0]\n",
    "        b = wb[0][1]\n",
    "        layer_name = cnn_layers[0][layer][0][0][0][0]\n",
    "        assert layer_name == expected_layer_name\n",
    "        return W, b\n",
    "\n",
    "    def _relu(conv2d_layer):\n",
    "        \"\"\"\n",
    "        Return the RELU function wrapped over a TensorFlow layer. Expects a\n",
    "        Conv2d layer input.\n",
    "        \"\"\"\n",
    "        return tf.nn.relu(conv2d_layer)\n",
    "\n",
    "    def _conv2d(prev_layer, layer, layer_name):\n",
    "        \"\"\"\n",
    "        Return the Conv2D layer using the weights, biases from the CNN\n",
    "        model at 'layer'.\n",
    "        \"\"\"\n",
    "        W, b = _weights(layer, layer_name)\n",
    "        W = tf.constant(W)\n",
    "        b = tf.constant(np.reshape(b, (b.size)))\n",
    "        return tf.nn.conv2d(prev_layer, filter=W, strides=[1, 1, 1, 1], padding='SAME') + b\n",
    "\n",
    "    def _conv2d_relu(prev_layer, layer, layer_name):\n",
    "        \"\"\"\n",
    "        Return the Conv2D + RELU layer using the weights, biases from the CNN\n",
    "        model at 'layer'.\n",
    "        \"\"\"\n",
    "        return _relu(_conv2d(prev_layer, layer, layer_name))\n",
    "\n",
    "    def _maxpool(prev_layer):\n",
    "        \"\"\"\n",
    "        Return the MaxPooling layer.\n",
    "        \"\"\"\n",
    "        return tf.nn.max_pool(prev_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    # The model is stored in a python dictionary where each variable name is the key and the corresponding value is a tensor containing that variable's value.\n",
    "    graph = {}\n",
    "    graph['input']   = tf.Variable(np.zeros((1, spectrogram_shape[0], spectrogram_shape[1])), dtype = 'float32')\n",
    "    graph['conv1_1']  = _conv2d_relu(graph['input'], 0, 'conv1_1')\n",
    "    graph['maxpool1'] = _maxpool(graph['conv1_'])\n",
    "    graph['conv2_1']  = _conv2d_relu(graph['maxpool1'], 5, 'conv2_1')\n",
    "    graph['maxpool2'] = _maxpool(graph['conv2_1'])\n",
    "    graph['conv3_1']  = _conv2d_relu(graph['maxpool2'], 10, 'conv3_1')\n",
    "    graph['maxpool3'] = _maxpool(graph['conv3_1'])\n",
    "    graph['conv4_1']  = _conv2d_relu(graph['maxpool3'], 19, 'conv4_1')\n",
    "    graph['maxpool4'] = _avgpool(graph['conv4_1'])\n",
    "    graph['conv5_1']  = _conv2d_relu(graph['maxpool4'], 28, 'conv5_1')\n",
    "    graph['maxpool5'] = _maxpool(graph['conv5_1'])\n",
    "    graph['conv6_1']  = _conv2d_relu(graph['maxpool5'], 28, 'conv6_1')\n",
    "    graph['maxpool6'] = _maxpool(graph['conv6_1'])\n",
    "    graph['conv7_1']  = _conv2d_relu(graph['maxpool6'], 28, 'conv7_1')\n",
    "    graph['maxpool7'] = _maxpool(graph['conv7_1'])\n",
    "    graph['conv8_1']  = _conv2d_relu(graph['maxpool7'], 28, 'conv8_1')\n",
    "    graph['maxpool8'] = _maxpool(graph['conv8_1'])\n",
    "    graph['conv9_1']  = _conv2d_relu(graph['maxpool8'], 28, 'conv9_1')\n",
    "    graph['maxpool9'] = _maxpool(graph['conv9_1'])\n",
    "    graph['conv10_1']  = _conv2d_relu(graph['maxpool9'], 28, 'conv10_1')\n",
    "    graph['maxpool10'] = _maxpool(graph['conv10_1'])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate noise spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noise_spectrogram(content_spectrogram, noise_ratio = CONFIG.NOISE_RATIO):\n",
    "    \"\"\"\n",
    "    Generates a noisy image by adding random noise to the content_spectrogram\n",
    "    \"\"\"\n",
    "    shape_spectrogram = content_spectrogram.shape\n",
    "    \n",
    "    # Generate a random noise_image\n",
    "    noise_spectrogram = np.random.uniform(-20, 20, (1, shape_spectrogram[0], shape_spectrogram[1])).astype('float32')\n",
    "    \n",
    "    # Set the input_image to be a weighted average of the content_image and a noise_image\n",
    "    input_spectrogram = noise_spectrogram * noise_ratio + content_spectrogram * (1 - noise_ratio)\n",
    "    \n",
    "    return input_spectrogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert audio to spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Function that parse audio from audio clip\n",
    "# Input: wav file name, t1 (start of phoeme), t2 (end of phoeme)\n",
    "# Returns: audio segments of individual phoeme between t1 and t2\n",
    "\"\"\"\n",
    "def parse_out_segment(audio_clip, t1, t2):\n",
    "    \n",
    "    # Grab audio segment between t1 and t2\n",
    "    # First grab the first t2 milliseconds\n",
    "    first_audio_segment = audio_clip[:t2]\n",
    "    \n",
    "    # Then grab the last t2-t1 milliseconds\n",
    "    phoeme_length = t2 - t1\n",
    "    audio_segment = first_audio_segment[-phoeme_length:]\n",
    "    \n",
    "    return audio_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Function that segments audio clip into smaller segments\n",
    "# Input: wav file name, flag to shift by 250ms, flag to change volume\n",
    "# Returns: dictionary of audio segments\n",
    "\"\"\"\n",
    "def segment_audio_clip(audio_file_name, wav_name):\n",
    "    \n",
    "    # Length is 1000ms = 1sec\n",
    "    segment_length = 500\n",
    "    \n",
    "    # Read the audio file\n",
    "    audio_clip = AudioSegment.from_wav(audio_file_name)\n",
    "    #print audio_clip.duration_seconds\n",
    "    \n",
    "    # Calculate the number of segments based on audio clip duration and segment length\n",
    "    audio_duration_ms = (audio_clip.duration_seconds)*1000    \n",
    "    num_segments = int(audio_duration_ms / segment_length)\n",
    "    #print num_segments\n",
    "    \n",
    "    # Segment the audio clip and save in dictionary\n",
    "    segment_dict = {}\n",
    "    \n",
    "    for i in range(num_segments):\n",
    "        key = wav_name + '_' + str(i)\n",
    "        segment_audio = parse_out_segment(audio_clip, i*1000, i*1000+segment_length)\n",
    "        segment_dict[key] = segment_audio\n",
    "    \n",
    "    return segment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Function that exports each audio segment to individual WAV files\n",
    "# Input: dictionary of phoemes audio clips\n",
    "# Returns: nothing\n",
    "\"\"\"\n",
    "def export_audio_segments(audio_segments_dict, wav_file_name, accent_id):\n",
    "    \n",
    "    for timestamp in audio_segments_dict:\n",
    "        export_file_name = accent_id + '_spectrograms\\\\' + timestamp + '.wav'\n",
    "        #export_file_name = timestamp + '.wav'\n",
    "        \n",
    "        #Exports to a wav file\n",
    "        audio_segment = audio_segments_dict[timestamp]\n",
    "        audio_segment.export(export_file_name, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Function that creates spectrogram of each WAV file\n",
    "# Input: none\n",
    "# Returns: nothing\n",
    "\"\"\"\n",
    "def convert_audio_to_spectrograms(accent_id, flag_pitch_shift):\n",
    "    reg_ex = accent_id + '_spectrograms\\*.wav'\n",
    "    wav_file_list = glob.glob(reg_ex)\n",
    "    #print wav_file_list\n",
    "    \n",
    "    spectrogram_list = []\n",
    "    \n",
    "    for wav_file in wav_file_list:\n",
    "        #print wav_file\n",
    "        \n",
    "        # Convert wav file to spectrogram (FFT)\n",
    "        samples, sampling_rate = librosa.load(wav_file)\n",
    "        \n",
    "        # Comput STFT of the audio\n",
    "        D = librosa.stft(samples)\n",
    "            \n",
    "        D_magnitude = np.abs(D)\n",
    "        #print (D_magnitude).shape\n",
    "        \n",
    "        D_reshape = np.reshape(D_magnitude,(205,110))\n",
    "        #print D_reshape.shape\n",
    "        \n",
    "        # Append to spectrogram list\n",
    "        spectrogram_list.append(D_reshape)\n",
    "        \n",
    "    return spectrogram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_spectrogram(input_wav_file):\n",
    "    #--------------------------------------------------\n",
    "    # SEGMENT WAV FILE\n",
    "    #--------------------------------------------------\n",
    "    audio_segments_dict = segment_audio_clip(wav_file, wav_name)\n",
    "            \n",
    "    #--------------------------------------------------\n",
    "    # EXPORT AS WAV FILES\n",
    "    #--------------------------------------------------\n",
    "    export_audio_segments(audio_segments_dict, wav_name, accent_id)\n",
    "    \n",
    "    #--------------------------------------------------\n",
    "    # CONVERT TO SPECTROGRAM\n",
    "    #--------------------------------------------------\n",
    "    spectrogram_list = convert_audio_to_spectrograms(accent_id)\n",
    "        \n",
    "    return spectrogram_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    STYLE_LAYERS = [\n",
    "        ('conv5_1', 0.05),\n",
    "        ('conv6_1', 0.30),\n",
    "        ('conv7_1', 0.30),\n",
    "        ('conv8_1', 0.30),\n",
    "        ('conv9_1', 0.05)]\n",
    "    \n",
    "    # Reset the graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Start interactive session\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    # Load the \"content\" spectrogram\n",
    "    content_spectrogram = convert_to_spectrogram('')\n",
    "\n",
    "    # Load the \"style\" spectrogram\n",
    "    style_spectrogram = convert_to_spectrogram('')\n",
    "\n",
    "    # Generate noisy spectrogram\n",
    "    generated_spectrogram = generate_noise_spectrogram(content_spectrogram)\n",
    "\n",
    "    # Assign the content image to be the input of the CNN model.  \n",
    "    sess.run(model['input'].assign(content_spectrogram))\n",
    "\n",
    "    # Load the model\n",
    "    # TODO : change this\n",
    "    model = load_cnn_model(\"pretrained_cnn_model.mat\", content_spectrogram.shape)\n",
    "\n",
    "    # Select the output tensor of layer conv4_2\n",
    "    out = model['conv3_2']\n",
    "\n",
    "    # Set a_C to be the hidden layer activation from the layer we have selected\n",
    "    a_C = sess.run(out)\n",
    "\n",
    "    # Set a_G to be the hidden layer activation from same layer. Here, a_G references model['conv4_2'] \n",
    "    # and isn't evaluated yet. Later in the code, we'll assign the image G as the model input, so that\n",
    "    # when we run the session, this will be the activations drawn from the appropriate layer, with G as input.\n",
    "    a_G = out\n",
    "\n",
    "    # Compute the content cost\n",
    "    J_content = compute_content_cost(a_C, a_G)\n",
    "\n",
    "    # Assign the input of the model to be the \"style\" image \n",
    "    sess.run(model['input'].assign(style_spectrogram))\n",
    "\n",
    "    # Compute the style cost\n",
    "    J_style = compute_style_cost(model, STYLE_LAYERS)\n",
    "\n",
    "    J = total_cost(J_content, J_style, 10, 40)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(2.0)\n",
    "\n",
    "    # Define train_step\n",
    "    train_step = optimizer.minimize(J)\n",
    "    \n",
    "    model_nn(sess, generated_spectrogram)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
